{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "#future is the missing compatibility layer between python 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs #to do word encoding\n",
    "import glob #finds all pathnames matching a pattern, like regex. to effectively search for large text\n",
    "import logging  #log events for libraries\n",
    "import multiprocessing #concurrency: a way of running multiple threads and having each thread run a differen process. it's a way of running your program faster\n",
    "import os #dealing with operating system, like reading file\n",
    "import pprint #pretty print, human readable\n",
    "import re #regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #importing natural language toolkit\n",
    "import gensim.models.word2vec as w2v #word 2 vec\n",
    "import sklearn.manifold #dimensionality reduction #the vectors are going to be multidimensional\n",
    "import numpy as np #math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up logging:\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk tokenize the sentences: exm:\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize, pos_tag\n",
    "\n",
    "sentence = ''''At eight oclock on Thursday morning Arthur didn't feel very good'''\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\") #pretrained tokenizer\n",
    "nltk.download(\"stopwords\") # words like, and, the an, a, of\n",
    "#we want to remove those words so the vectors we create are more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_filenames = sorted(glob.glob(\"*.txt\"))\n",
    "\n",
    "print(\"Found books:\")\n",
    "book_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a raw corpus, we'll add all text to this file\n",
    "corpus_raw = u'' #we start with u, because it's a unicode, and we want to convert into utf-8\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "    print(\"corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "    print()\n",
    "    \n",
    "#for each book, read it, open it in utf-8 format, add it to the corpus_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into list of words\n",
    "#remove unnecessary characters, split into words\n",
    "\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\", \" \", raw)\n",
    "    words = clean.split()\n",
    "    return words\n",
    "#\"[^a-zA-Z]\": match all the stings that contain a non-letter\n",
    "#\"^[a-zA-Z]\": means match all the strings with a letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_sentences[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_to_wordlist(raw_sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Model\n",
    "num_features = 300 #dimensionality of the resulting word vectors\n",
    "min_word_count = 3\n",
    "num_workers = multiprocessing.cpu_count()# number of threads to run in parallel\n",
    "context_size = 7\n",
    "downsampling = 1e-3 #downsamle setting for frequent words = how often to use\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec = w2v.Word2Vec(sg=1,\n",
    "                          seed=seed,\n",
    "                          workers=num_workers, \n",
    "                          size=num_features, \n",
    "                          min_count=min_word_count, \n",
    "                          window=context_size, \n",
    "                          sample=downsampling)\n",
    "#sg=1: If 1, skip-gram is employed; otherwise, CBOW is used.\n",
    "#workers=num_workers:Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "#size=num_features: Dimensionality of the feature vectors.\n",
    "#min_count=min_word_count:  Ignores all words with total frequency lower than this.\n",
    "#window=context_size: The maximum distance between the current and predicted word within a sentence.\n",
    "#sample=downsampling:  The threshold for configuring which higher-frequency words are randomly downsampled, useful range is\n",
    "(0, 1e-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec.build_vocab(sentences) # the new words in `sentences` will be added to model's vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(thrones2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start training\n",
    "thrones2vec.train(sentences, total_words=token_count, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file\n",
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec.save(os.path.join(\"trained\", \"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "thrones2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_vectors_matrix = thrones2vec.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the big picture\n",
    "points = pd.DataFrame(\n",
    "    [\n",
    "        (word, coords[0], coords[1])\n",
    "        for word, coords in [\n",
    "            (word, all_word_vectors_matrix_2d[thrones2vec.wv.vocab[word].index])\n",
    "            for word in thrones2vec.wv.vocab\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"word\", \"x\", \"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.plot.scatter(\"x\", \"y\", s=10, figsize=(25, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#details\n",
    "def plot_region(x_bounds, y_bounds):\n",
    "    slice = points[\n",
    "        (x_bounds[0] <= points.x) &\n",
    "        (points.x <= x_bounds[1]) & \n",
    "        (y_bounds[0] <= points.y) &\n",
    "        (points.y <= y_bounds[1])\n",
    "    ]\n",
    "    \n",
    "    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n",
    "    for i, point in slice.iterrows():\n",
    "        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Range of t-sne viz in x = {}, {} and y = {}, {}\" .format(min(points.x), max(points.x), min(points.y), max(points.y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Semantic Similarities\n",
    "thrones2vec.most_similar(\"Stark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec.most_similar(\"Dragons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec.most_similar(\"direwolf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_similarity_cosmul(start1, end1, end2):\n",
    "    similarities = thrones2vec.most_similar_cosmul(\n",
    "        positive=[end2, start1],\n",
    "        negative=[end1]\n",
    "    )\n",
    "    start2 = similarities[0][0]\n",
    "    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n",
    "    return start2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_similarity_cosmul(\"Stark\", \"Winterfell\", \"Martell\") #Leader\n",
    "nearest_similarity_cosmul(\"Stark\", \"Winterfell\", \"Bolton\")  # Head of families\n",
    "nearest_similarity_cosmul(\"Arya\", \"Horseface\", \"Daenerys\") # Nicknames\n",
    "nearest_similarity_cosmul(\"Stark\", \"Winterfell\", \"Riverrun\") # Leaders \n",
    "nearest_similarity_cosmul(\"Jaime\", \"sword\", \"wine\")\n",
    "nearest_similarity_cosmul(\"Tyrion\", \"wine\", \"dragons\")\n",
    "nearest_similarity_cosmul(\"Arya\", \"Nymeria\", \"dragons\") # Mystic/Extinct creatures\n",
    "nearest_similarity_cosmul(\"Snow\", \"Jon\", \"Ellaria\") # Bastards in North/Dorneb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec.wv.doesnt_match(\"Tyrion Daenerys Gendry Bran Jon\".split())  #Iron Throne contenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec.wv.doesnt_match(\"Jaime Cersei Robert\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec.wv.doesnt_match(\"Ramsay Jon Ellaria \".split())  #Snow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
