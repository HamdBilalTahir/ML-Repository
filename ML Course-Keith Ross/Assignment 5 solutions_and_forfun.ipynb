{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning Spring 2018 Assignment 5\n",
    "You are given a data set with 5000 handwritten digits and their corresponding\n",
    "labels. Each training example is a 20 pixel by 20 pixel grayscale image of the\n",
    "digit. Each pixel is represented by a number indicating the grayscale intensity at\n",
    "that location. Thus, your neural network will have 400 inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pylab as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded in shape of (5000, 400)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEBCAYAAABojF4hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1184a96d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_1 is loaded in shape: (25, 401)\n",
      "W_2 is loaded in shape: (10, 26)\n"
     ]
    }
   ],
   "source": [
    "## load image data\n",
    "infile = open('/Users/xg7/Desktop/teachingstuff/MachineLearning2018/assigment/Assignment5/ps5_data.csv','r')\n",
    "img_data = infile.read().strip().split('\\n')\n",
    "img = [map(float,a.strip().split(',')) for a in img_data]\n",
    "pixels = []\n",
    "for p in img:\n",
    "    pixels += p\n",
    "data = np.reshape(pixels,(5000,400))\n",
    "print('data loaded in shape of', data.shape)\n",
    "## loda labels\n",
    "infile = open('/Users/xg7/Desktop/teachingstuff/MachineLearning2018/assigment/Assignment5/ps5_data-labels.csv','r')\n",
    "img_label = infile.read().strip().split('\\n')\n",
    "Label_ = [map(int,a.strip().split(',')) for a in img_label]\n",
    "Label = []\n",
    "for l in Label_:\n",
    "    Label += l\n",
    "    \n",
    "\n",
    "## visualizing \n",
    "plt.figure()\n",
    "plt.suptitle('10 handwritten numbers')\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.title(Label[i*500])\n",
    "    plt.imshow(np.reshape(data[i*500],(20,20),order='F'), cmap=plt.cm.gray)\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "##load weights\n",
    "infile = open('/Users/xg7/Desktop/teachingstuff/MachineLearning2018/assigment/Assignment5/ps5_theta1.csv','r')\n",
    "theta1 = infile.read().strip().split('\\n')\n",
    "W_1_ = [map(float,a.strip().split(',')) for a in theta1]\n",
    "W_1 = []\n",
    "for w in W_1_:\n",
    "    W_1 += w\n",
    "W_1 = np.reshape(W_1, (25, 401))\n",
    "print(\"W_1 is loaded in shape:\", W_1.shape)\n",
    "\n",
    "infile = open('/Users/xg7/Desktop/teachingstuff/MachineLearning2018/assigment/Assignment5/ps5_theta2.csv','r')\n",
    "theta2 = infile.read().strip().split('\\n')\n",
    "W_2_ = [map(float,a.strip().split(',')) for a in theta2]\n",
    "W_2 = []\n",
    "for w in W_2_:\n",
    "    W_2 += w\n",
    "W_2 = np.reshape(W_2, (10, 26))\n",
    "print(\"W_2 is loaded in shape:\", W_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A unit in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer_unit(x, w):\n",
    "    # x: a vector in size of 400 x 1\n",
    "    # w: a vector in size of 401 x 1 (including bias as w[0])\n",
    "    z = w[0]\n",
    "    for i in range(len(x)):\n",
    "        z += x[i]*w[i+1]\n",
    "    e = math.exp(-z)\n",
    "    a = 1/(1+e)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    # z: a vector in size of 10 x 1\n",
    "    e = []\n",
    "    prob = []\n",
    "    for i, z_i in enumerate(z):\n",
    "        try:\n",
    "            e.append(math.exp(z_i))\n",
    "        except OverflowError:\n",
    "            prob = [0 for j in range(len(z))]\n",
    "            prob[i] = 1\n",
    "            return prob\n",
    "    total = sum(e)\n",
    "    for e_i in e:\n",
    "        prob.append(e_i/total)\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_layer(a_h, W):\n",
    "    # a_h: a vector, 25 x 1, is the output of the hidden layer\n",
    "    # W: a matrix, 10 x 26, is the weights between the hidden layer and the softmax layer\n",
    "    a = []\n",
    "    for i, w_i in enumerate(W):\n",
    "        z_j = w_i[0]\n",
    "        for j, w_ij in enumerate(w_i[1:]):\n",
    "            z_j += a_h[j]*w_ij\n",
    "        a.append(z_j)\n",
    "    return softmax(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(x, W_1, W_2):\n",
    "    # x: a 400 x 1 vector, one sample\n",
    "    # W_1: 25 x 401 matrix, the weights between input and hidden layer\n",
    "    # W_2: 10 x 26 matrix, the weights between hidden and softmax layer\n",
    "    a_h = []\n",
    "    for w_i in W_1:\n",
    "        a = hidden_layer_unit(x, w_i)\n",
    "        a_h.append(a)\n",
    "    p = softmax_layer(a_h, W_2)\n",
    "    return p "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_classifier(x, W_1, W_2):\n",
    "    # x: a 400 x 1 vector, one sample\n",
    "    # W_1: 25 x 401 matrix, the weights between input and hidden layer\n",
    "    # W_2: 10 x 26 matrix, the weights between hidden and softmax layer\n",
    "    p = network(x, W_1, W_2)\n",
    "    pred_label = p.index(max(p))\n",
    "    return pred_label + 1 # note: 0 is labeled as 10\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(data, W_1, W_2, Label):\n",
    "    # data: the whole data set\n",
    "    # W_1: 25 x 401 matrix, the weights between input and hidden layer\n",
    "    # W_2: 10 x 26 matrix, the weights between hidden and softmax layer\n",
    "    # label: the ground truth\n",
    "    errors = 0\n",
    "    for i, img in enumerate(data):\n",
    "        if img_classifier(img, W_1, W_2) != Label[i]:\n",
    "            errors += 1\n",
    "    return errors/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE loss of softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_softmax(p, label):\n",
    "    # p: 10 x 1 vector, the output of the softmax\n",
    "    # label: scalar, the ground truth of a sample\n",
    "    return -math.log(p[label-1])\n",
    "\n",
    "\n",
    "def MLE_loss(data, W1, W2, Label):\n",
    "    L = 0\n",
    "    for i, img in enumerate(data):\n",
    "        p = network(img, W1, W2)\n",
    "        L += loss_softmax(p, Label[i])\n",
    "    return L/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the main routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error rate is: 0.0248\n",
      "The MLE loss is: 0.08688856037475012\n"
     ]
    }
   ],
   "source": [
    "print(\"The error rate is:\", error_rate(data, W_1,W_2, Label))\n",
    "print(\"The MLE loss is:\", MLE_loss(data, W_1, W_2, Label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Fun: Build a network with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator data\n",
    "In the data we have, the samples are listed in the order according to the classes from 0 to 9, each contains 500 examples. You need to re-arrange the data points, for example, take the first 300 for each class for training, and take the next 100 for validation, and the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data set is in the shape: (4000, 400)\n",
      "test data set is in the shape: (1000, 400)\n",
      "The train label is in shape: (4000,)\n",
      "The test label is in shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "## data re-arrangement\n",
    "train_num_per_class = 400\n",
    "train_data = np.empty([train_num_per_class*10, 400])\n",
    "train_label = np.empty(train_num_per_class*10)\n",
    "for i in range(10):\n",
    "    train_data[i*train_num_per_class:(i+1)*train_num_per_class] = data[i*500:i*500+train_num_per_class]\n",
    "    train_label[i*train_num_per_class:(i+1)*train_num_per_class] = Label[i*500:i*500+train_num_per_class]\n",
    "print('train data set is in the shape:', train_data.shape)\n",
    "\n",
    "test_num_per_class = 100\n",
    "test_data = np.empty([test_num_per_class*10, 400])\n",
    "test_label = np.empty(test_num_per_class*10)\n",
    "for i in range(10):\n",
    "    test_data[i*test_num_per_class:(i+1)*test_num_per_class] = data[i*500+train_num_per_class:i*500+train_num_per_class+test_num_per_class]\n",
    "    test_label[i*test_num_per_class:(i+1)*test_num_per_class] = Label[i*500+train_num_per_class:i*500+train_num_per_class+test_num_per_class]\n",
    "print('test data set is in the shape:', test_data.shape)\n",
    "\n",
    "nb_classes = 10\n",
    "train_label = train_label - 1\n",
    "test_label = test_label - 1\n",
    "print('The train label is in shape:', train_label.shape)\n",
    "print('The test label is in shape:', test_label.shape)\n",
    "#print(test_label[i*test_num_per_class:(i+1)*test_num_per_class])\n",
    "#print(Label[i*500+train_num_per_class:i*500+train_num_per_class+test_num_per_class])\n",
    "train_label = np_utils.to_categorical(train_label, nb_classes)\n",
    "test_label = np_utils.to_categorical(test_label, nb_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 2s 442us/step - loss: 2.2917 - acc: 0.1585 - val_loss: 2.1589 - val_acc: 0.2950\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 1s 313us/step - loss: 2.0714 - acc: 0.4193 - val_loss: 1.9793 - val_acc: 0.5020\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 1s 305us/step - loss: 1.9104 - acc: 0.5945 - val_loss: 1.8321 - val_acc: 0.6370\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 1s 302us/step - loss: 1.7735 - acc: 0.6720 - val_loss: 1.7042 - val_acc: 0.7080\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 1s 302us/step - loss: 1.6537 - acc: 0.7050 - val_loss: 1.5930 - val_acc: 0.7250\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 1s 307us/step - loss: 1.5482 - acc: 0.7260 - val_loss: 1.4945 - val_acc: 0.7340\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 1s 315us/step - loss: 1.4538 - acc: 0.7390 - val_loss: 1.4068 - val_acc: 0.7450\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 1s 370us/step - loss: 1.3688 - acc: 0.7492 - val_loss: 1.3267 - val_acc: 0.7520\n",
      "Epoch 9/100\n",
      "4000/4000 [==============================] - 1s 342us/step - loss: 1.2909 - acc: 0.7635 - val_loss: 1.2538 - val_acc: 0.7680\n",
      "Epoch 10/100\n",
      "4000/4000 [==============================] - 1s 351us/step - loss: 1.2193 - acc: 0.7785 - val_loss: 1.1869 - val_acc: 0.7740\n",
      "Epoch 11/100\n",
      "4000/4000 [==============================] - 1s 323us/step - loss: 1.1533 - acc: 0.7925 - val_loss: 1.1247 - val_acc: 0.7970\n",
      "Epoch 12/100\n",
      "4000/4000 [==============================] - 1s 331us/step - loss: 1.0921 - acc: 0.8050 - val_loss: 1.0684 - val_acc: 0.8050\n",
      "Epoch 13/100\n",
      "4000/4000 [==============================] - 1s 335us/step - loss: 1.0363 - acc: 0.8177 - val_loss: 1.0166 - val_acc: 0.8190\n",
      "Epoch 14/100\n",
      "4000/4000 [==============================] - 1s 332us/step - loss: 0.9853 - acc: 0.8267 - val_loss: 0.9688 - val_acc: 0.8310\n",
      "Epoch 15/100\n",
      "4000/4000 [==============================] - 1s 328us/step - loss: 0.9380 - acc: 0.8367 - val_loss: 0.9255 - val_acc: 0.8340\n",
      "Epoch 16/100\n",
      "4000/4000 [==============================] - 1s 316us/step - loss: 0.8950 - acc: 0.8437 - val_loss: 0.8864 - val_acc: 0.8420\n",
      "Epoch 17/100\n",
      "4000/4000 [==============================] - 1s 330us/step - loss: 0.8556 - acc: 0.8485 - val_loss: 0.8499 - val_acc: 0.8440\n",
      "Epoch 18/100\n",
      "4000/4000 [==============================] - 1s 358us/step - loss: 0.8189 - acc: 0.8545 - val_loss: 0.8156 - val_acc: 0.8510\n",
      "Epoch 19/100\n",
      "4000/4000 [==============================] - 2s 380us/step - loss: 0.7851 - acc: 0.8582 - val_loss: 0.7849 - val_acc: 0.8610\n",
      "Epoch 20/100\n",
      "4000/4000 [==============================] - 1s 329us/step - loss: 0.7536 - acc: 0.8617 - val_loss: 0.7565 - val_acc: 0.8620\n",
      "Epoch 21/100\n",
      "4000/4000 [==============================] - 1s 317us/step - loss: 0.7249 - acc: 0.8660 - val_loss: 0.7299 - val_acc: 0.8610\n",
      "Epoch 22/100\n",
      "4000/4000 [==============================] - 1s 334us/step - loss: 0.6977 - acc: 0.8705 - val_loss: 0.7055 - val_acc: 0.8610\n",
      "Epoch 23/100\n",
      "4000/4000 [==============================] - 1s 327us/step - loss: 0.6728 - acc: 0.8730 - val_loss: 0.6828 - val_acc: 0.8660\n",
      "Epoch 24/100\n",
      "4000/4000 [==============================] - 1s 329us/step - loss: 0.6501 - acc: 0.8735 - val_loss: 0.6623 - val_acc: 0.8690\n",
      "Epoch 25/100\n",
      "4000/4000 [==============================] - 1s 328us/step - loss: 0.6286 - acc: 0.8767 - val_loss: 0.6425 - val_acc: 0.8730\n",
      "Epoch 26/100\n",
      "4000/4000 [==============================] - 1s 355us/step - loss: 0.6089 - acc: 0.8810 - val_loss: 0.6246 - val_acc: 0.8760\n",
      "Epoch 27/100\n",
      "4000/4000 [==============================] - 1s 309us/step - loss: 0.5905 - acc: 0.8827 - val_loss: 0.6077 - val_acc: 0.8770\n",
      "Epoch 28/100\n",
      "4000/4000 [==============================] - 1s 302us/step - loss: 0.5732 - acc: 0.8847 - val_loss: 0.5918 - val_acc: 0.8800\n",
      "Epoch 29/100\n",
      "4000/4000 [==============================] - 1s 299us/step - loss: 0.5571 - acc: 0.8870 - val_loss: 0.5778 - val_acc: 0.8790\n",
      "Epoch 30/100\n",
      "4000/4000 [==============================] - 1s 329us/step - loss: 0.5423 - acc: 0.8887 - val_loss: 0.5645 - val_acc: 0.8790\n",
      "Epoch 31/100\n",
      "4000/4000 [==============================] - 2s 383us/step - loss: 0.5282 - acc: 0.8892 - val_loss: 0.5517 - val_acc: 0.8810\n",
      "Epoch 32/100\n",
      "4000/4000 [==============================] - 1s 358us/step - loss: 0.5149 - acc: 0.8905 - val_loss: 0.5399 - val_acc: 0.8820\n",
      "Epoch 33/100\n",
      "4000/4000 [==============================] - 2s 507us/step - loss: 0.5025 - acc: 0.8920 - val_loss: 0.5286 - val_acc: 0.8830\n",
      "Epoch 34/100\n",
      "4000/4000 [==============================] - 2s 405us/step - loss: 0.4909 - acc: 0.8927 - val_loss: 0.5179 - val_acc: 0.8830\n",
      "Epoch 35/100\n",
      "4000/4000 [==============================] - 1s 313us/step - loss: 0.4800 - acc: 0.8950 - val_loss: 0.5085 - val_acc: 0.8850\n",
      "Epoch 36/100\n",
      "4000/4000 [==============================] - 1s 308us/step - loss: 0.4696 - acc: 0.8967 - val_loss: 0.4996 - val_acc: 0.8860\n",
      "Epoch 37/100\n",
      "4000/4000 [==============================] - 1s 300us/step - loss: 0.4598 - acc: 0.8980 - val_loss: 0.4907 - val_acc: 0.8860\n",
      "Epoch 38/100\n",
      "4000/4000 [==============================] - 1s 296us/step - loss: 0.4506 - acc: 0.8992 - val_loss: 0.4824 - val_acc: 0.8860\n",
      "Epoch 39/100\n",
      "4000/4000 [==============================] - 1s 313us/step - loss: 0.4418 - acc: 0.9002 - val_loss: 0.4745 - val_acc: 0.8850\n",
      "Epoch 40/100\n",
      "4000/4000 [==============================] - 1s 346us/step - loss: 0.4334 - acc: 0.9017 - val_loss: 0.4673 - val_acc: 0.8860\n",
      "Epoch 41/100\n",
      "4000/4000 [==============================] - 1s 323us/step - loss: 0.4256 - acc: 0.9020 - val_loss: 0.4603 - val_acc: 0.8870\n",
      "Epoch 42/100\n",
      "4000/4000 [==============================] - 1s 327us/step - loss: 0.4181 - acc: 0.9035 - val_loss: 0.4539 - val_acc: 0.8890\n",
      "Epoch 43/100\n",
      "4000/4000 [==============================] - 1s 334us/step - loss: 0.4110 - acc: 0.9050 - val_loss: 0.4473 - val_acc: 0.8880\n",
      "Epoch 44/100\n",
      "4000/4000 [==============================] - 1s 361us/step - loss: 0.4042 - acc: 0.9055 - val_loss: 0.4416 - val_acc: 0.8870\n",
      "Epoch 45/100\n",
      "4000/4000 [==============================] - 1s 305us/step - loss: 0.3978 - acc: 0.9072 - val_loss: 0.4356 - val_acc: 0.8900\n",
      "Epoch 46/100\n",
      "4000/4000 [==============================] - 1s 352us/step - loss: 0.3917 - acc: 0.9090 - val_loss: 0.4307 - val_acc: 0.8890\n",
      "Epoch 47/100\n",
      "4000/4000 [==============================] - 1s 316us/step - loss: 0.3859 - acc: 0.9100 - val_loss: 0.4256 - val_acc: 0.8900\n",
      "Epoch 48/100\n",
      "4000/4000 [==============================] - 1s 315us/step - loss: 0.3803 - acc: 0.9107 - val_loss: 0.4207 - val_acc: 0.8910\n",
      "Epoch 49/100\n",
      "4000/4000 [==============================] - 1s 326us/step - loss: 0.3750 - acc: 0.9112 - val_loss: 0.4160 - val_acc: 0.8920\n",
      "Epoch 50/100\n",
      "4000/4000 [==============================] - 1s 330us/step - loss: 0.3699 - acc: 0.9115 - val_loss: 0.4117 - val_acc: 0.8920\n",
      "Epoch 51/100\n",
      "4000/4000 [==============================] - 1s 317us/step - loss: 0.3650 - acc: 0.9127 - val_loss: 0.4072 - val_acc: 0.8920\n",
      "Epoch 52/100\n",
      "4000/4000 [==============================] - 1s 315us/step - loss: 0.3605 - acc: 0.9122 - val_loss: 0.4034 - val_acc: 0.8930\n",
      "Epoch 53/100\n",
      "4000/4000 [==============================] - 1s 304us/step - loss: 0.3560 - acc: 0.9140 - val_loss: 0.3993 - val_acc: 0.8920\n",
      "Epoch 54/100\n",
      "4000/4000 [==============================] - 1s 325us/step - loss: 0.3516 - acc: 0.9140 - val_loss: 0.3954 - val_acc: 0.8940\n",
      "Epoch 55/100\n",
      "4000/4000 [==============================] - 1s 312us/step - loss: 0.3475 - acc: 0.9147 - val_loss: 0.3918 - val_acc: 0.8940\n",
      "Epoch 56/100\n",
      "4000/4000 [==============================] - 1s 312us/step - loss: 0.3435 - acc: 0.9155 - val_loss: 0.3885 - val_acc: 0.8940\n",
      "Epoch 57/100\n",
      "4000/4000 [==============================] - 1s 325us/step - loss: 0.3397 - acc: 0.9175 - val_loss: 0.3853 - val_acc: 0.8940\n",
      "Epoch 58/100\n",
      "4000/4000 [==============================] - 1s 307us/step - loss: 0.3359 - acc: 0.9182 - val_loss: 0.3825 - val_acc: 0.8930\n",
      "Epoch 59/100\n",
      "4000/4000 [==============================] - 1s 311us/step - loss: 0.3324 - acc: 0.9182 - val_loss: 0.3793 - val_acc: 0.8920\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 1s 299us/step - loss: 0.3289 - acc: 0.9190 - val_loss: 0.3765 - val_acc: 0.8930\n",
      "Epoch 61/100\n",
      "4000/4000 [==============================] - 1s 302us/step - loss: 0.3256 - acc: 0.9195 - val_loss: 0.3736 - val_acc: 0.8910\n",
      "Epoch 62/100\n",
      "4000/4000 [==============================] - 1s 311us/step - loss: 0.3224 - acc: 0.9195 - val_loss: 0.3706 - val_acc: 0.8940\n",
      "Epoch 63/100\n",
      "4000/4000 [==============================] - 1s 296us/step - loss: 0.3192 - acc: 0.9197 - val_loss: 0.3681 - val_acc: 0.8960\n",
      "Epoch 64/100\n",
      "4000/4000 [==============================] - 1s 302us/step - loss: 0.3162 - acc: 0.9205 - val_loss: 0.3659 - val_acc: 0.8950\n",
      "Epoch 65/100\n",
      "4000/4000 [==============================] - 1s 305us/step - loss: 0.3133 - acc: 0.9210 - val_loss: 0.3636 - val_acc: 0.8990\n",
      "Epoch 66/100\n",
      "4000/4000 [==============================] - 1s 306us/step - loss: 0.3105 - acc: 0.9212 - val_loss: 0.3611 - val_acc: 0.8990\n",
      "Epoch 67/100\n",
      "4000/4000 [==============================] - 1s 316us/step - loss: 0.3078 - acc: 0.9212 - val_loss: 0.3589 - val_acc: 0.8980\n",
      "Epoch 68/100\n",
      "4000/4000 [==============================] - 1s 322us/step - loss: 0.3051 - acc: 0.9222 - val_loss: 0.3568 - val_acc: 0.8980\n",
      "Epoch 69/100\n",
      "4000/4000 [==============================] - 1s 312us/step - loss: 0.3025 - acc: 0.9212 - val_loss: 0.3548 - val_acc: 0.9000\n",
      "Epoch 70/100\n",
      "4000/4000 [==============================] - 1s 324us/step - loss: 0.3001 - acc: 0.9237 - val_loss: 0.3527 - val_acc: 0.8990\n",
      "Epoch 71/100\n",
      "4000/4000 [==============================] - 1s 325us/step - loss: 0.2977 - acc: 0.9232 - val_loss: 0.3502 - val_acc: 0.8990\n",
      "Epoch 72/100\n",
      "4000/4000 [==============================] - 1s 316us/step - loss: 0.2954 - acc: 0.9255 - val_loss: 0.3486 - val_acc: 0.9010\n",
      "Epoch 73/100\n",
      "4000/4000 [==============================] - 1s 309us/step - loss: 0.2931 - acc: 0.9247 - val_loss: 0.3466 - val_acc: 0.8990\n",
      "Epoch 74/100\n",
      "4000/4000 [==============================] - 1s 307us/step - loss: 0.2910 - acc: 0.9262 - val_loss: 0.3448 - val_acc: 0.8990\n",
      "Epoch 75/100\n",
      "4000/4000 [==============================] - 1s 314us/step - loss: 0.2888 - acc: 0.9257 - val_loss: 0.3432 - val_acc: 0.9000\n",
      "Epoch 76/100\n",
      "4000/4000 [==============================] - 1s 296us/step - loss: 0.2867 - acc: 0.9265 - val_loss: 0.3414 - val_acc: 0.9010\n",
      "Epoch 77/100\n",
      "4000/4000 [==============================] - 1s 322us/step - loss: 0.2847 - acc: 0.9262 - val_loss: 0.3396 - val_acc: 0.9030\n",
      "Epoch 78/100\n",
      "4000/4000 [==============================] - 1s 312us/step - loss: 0.2827 - acc: 0.9272 - val_loss: 0.3376 - val_acc: 0.9030\n",
      "Epoch 79/100\n",
      "4000/4000 [==============================] - 1s 313us/step - loss: 0.2806 - acc: 0.9272 - val_loss: 0.3365 - val_acc: 0.9050\n",
      "Epoch 80/100\n",
      "4000/4000 [==============================] - 1s 299us/step - loss: 0.2788 - acc: 0.9280 - val_loss: 0.3349 - val_acc: 0.9050\n",
      "Epoch 81/100\n",
      "4000/4000 [==============================] - 1s 308us/step - loss: 0.2769 - acc: 0.9287 - val_loss: 0.3337 - val_acc: 0.9040\n",
      "Epoch 82/100\n",
      "4000/4000 [==============================] - 1s 312us/step - loss: 0.2752 - acc: 0.9287 - val_loss: 0.3322 - val_acc: 0.9060\n",
      "Epoch 83/100\n",
      "4000/4000 [==============================] - 1s 310us/step - loss: 0.2733 - acc: 0.9285 - val_loss: 0.3313 - val_acc: 0.9070\n",
      "Epoch 84/100\n",
      "4000/4000 [==============================] - 1s 311us/step - loss: 0.2717 - acc: 0.9305 - val_loss: 0.3296 - val_acc: 0.9080\n",
      "Epoch 85/100\n",
      "4000/4000 [==============================] - 1s 308us/step - loss: 0.2700 - acc: 0.9302 - val_loss: 0.3284 - val_acc: 0.9090\n",
      "Epoch 86/100\n",
      "4000/4000 [==============================] - 1s 306us/step - loss: 0.2683 - acc: 0.9300 - val_loss: 0.3272 - val_acc: 0.9080\n",
      "Epoch 87/100\n",
      "4000/4000 [==============================] - 1s 306us/step - loss: 0.2667 - acc: 0.9305 - val_loss: 0.3258 - val_acc: 0.9070\n",
      "Epoch 88/100\n",
      "4000/4000 [==============================] - 1s 305us/step - loss: 0.2651 - acc: 0.9300 - val_loss: 0.3248 - val_acc: 0.9110\n",
      "Epoch 89/100\n",
      "4000/4000 [==============================] - 1s 311us/step - loss: 0.2635 - acc: 0.9312 - val_loss: 0.3237 - val_acc: 0.9110\n",
      "Epoch 90/100\n",
      "4000/4000 [==============================] - 1s 321us/step - loss: 0.2621 - acc: 0.9320 - val_loss: 0.3225 - val_acc: 0.9110\n",
      "Epoch 91/100\n",
      "4000/4000 [==============================] - 1s 317us/step - loss: 0.2606 - acc: 0.9310 - val_loss: 0.3214 - val_acc: 0.9120\n",
      "Epoch 92/100\n",
      "4000/4000 [==============================] - 1s 316us/step - loss: 0.2591 - acc: 0.9322 - val_loss: 0.3206 - val_acc: 0.9120\n",
      "Epoch 93/100\n",
      "4000/4000 [==============================] - 1s 312us/step - loss: 0.2577 - acc: 0.9315 - val_loss: 0.3192 - val_acc: 0.9120\n",
      "Epoch 94/100\n",
      "4000/4000 [==============================] - 1s 309us/step - loss: 0.2563 - acc: 0.9320 - val_loss: 0.3180 - val_acc: 0.9120\n",
      "Epoch 95/100\n",
      "4000/4000 [==============================] - 1s 312us/step - loss: 0.2549 - acc: 0.9322 - val_loss: 0.3171 - val_acc: 0.9130\n",
      "Epoch 96/100\n",
      "4000/4000 [==============================] - 1s 317us/step - loss: 0.2536 - acc: 0.9322 - val_loss: 0.3157 - val_acc: 0.9120\n",
      "Epoch 97/100\n",
      "4000/4000 [==============================] - 1s 319us/step - loss: 0.2521 - acc: 0.9322 - val_loss: 0.3149 - val_acc: 0.9120\n",
      "Epoch 98/100\n",
      "4000/4000 [==============================] - 1s 308us/step - loss: 0.2509 - acc: 0.9325 - val_loss: 0.3139 - val_acc: 0.9110\n",
      "Epoch 99/100\n",
      "4000/4000 [==============================] - 1s 312us/step - loss: 0.2496 - acc: 0.9337 - val_loss: 0.3132 - val_acc: 0.9130\n",
      "Epoch 100/100\n",
      "4000/4000 [==============================] - 1s 335us/step - loss: 0.2482 - acc: 0.9335 - val_loss: 0.3122 - val_acc: 0.9110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x117be2400>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(26, activation='sigmoid', use_bias=True, input_shape=(400,)))\n",
    "model.add(Dense(10, activation='softmax', use_bias=True))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "model.fit(train_data, train_label, batch_size=10, shuffle=True, epochs=100, validation_data=(test_data, test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.319417184949\n",
      "Test accuracy: 0.915\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_data, test_label, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
